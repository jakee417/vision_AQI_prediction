{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AQI file from EPA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14079,
     "status": "ok",
     "timestamp": 1604296878579,
     "user": {
      "displayName": "Robert Schmidt",
      "photoUrl": "",
      "userId": "01672488058839739461"
     },
     "user_tz": 480
    },
    "id": "Ad3SR6C-eqbS"
   },
   "outputs": [],
   "source": [
    "# Libraries and Preliminaries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "project_root_dir = 'put_your_root_directory_here/aqi_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB3AkzC3BehC"
   },
   "source": [
    "The following code builds the AQI EPA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14078,
     "status": "ok",
     "timestamp": 1604296878580,
     "user": {
      "displayName": "Robert Schmidt",
      "photoUrl": "",
      "userId": "01672488058839739461"
     },
     "user_tz": 480
    },
    "id": "dAezHd3AfG6S"
   },
   "outputs": [],
   "source": [
    "aqi_classes = ['Good', 'Moderate', 'Unhealthy for sensitive groups', 'Unhealthy', 'Very unhealthy', 'Hazardous']\n",
    "aqi_cutoffs = np.array([0, 50, 100, 150, 200, 300, np.inf])\n",
    "year_list = [2016, 2017, 2018, 2019, 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUqAw_oMciLX"
   },
   "outputs": [],
   "source": [
    "def load_data_in_year_range(base_file, file_type, year_range):\n",
    "    \"\"\"\n",
    "    Loads all the data in a given year range for the specified pollutant.\n",
    "    \n",
    "    Args:\n",
    "        base_file: file prefix for loading\n",
    "        file_type: one of 'ozone' or 'pm2.5' (can add more if using every pollutant)\n",
    "        year_range: list of years to look for in the directory\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with dates, site ID, and daily AQI value for the specified range and particle.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    for year in year_range:\n",
    "        file_list.append(pd.read_csv(base_file + '_' + file_type + '_' + str(year) + '.csv'))\n",
    "\n",
    "    final_file = reduce(lambda x, y: pd.concat([x, y], ignore_index = True), file_list)\n",
    "    final_file = final_file[[\"Date\", \"Site ID\", \"DAILY_AQI_VALUE\"]] # extract only these columns\n",
    "    renamed_column = file_type + '_AQI'\n",
    "    final_file = final_file.rename(columns = {'Date': 'Date', 'Site ID': 'Site_ID','DAILY_AQI_VALUE': renamed_column})\n",
    "    final_file['Date'] = pd.to_datetime(final_file['Date'])\n",
    "    return final_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d04I6_MssxH0"
   },
   "outputs": [],
   "source": [
    "def clean_AQI_csv(aqi_classes, aqi_cutoffs, year_list):\n",
    "    \"\"\"\n",
    "    Clean all of the AQI files in the current AQI directory.\n",
    "    \n",
    "    Args:\n",
    "        aqi_classes: list of air quality class names under consideration\n",
    "        aqi_cutoffs: numpy array of the cutoffs corresponding to the above classes\n",
    "        year_list: list of years to be loaded\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with AQI information for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_file = os.path.join(project_root_dir, 'all_sites')\n",
    "\n",
    "    # Load datasets\n",
    "    ozone_data = load_data_in_year_range(base_file, 'ozone', year_list)\n",
    "    pm_data = load_data_in_year_range(base_file, 'pm2.5', year_list)\n",
    "\n",
    "    # Merge the datasets and calculate the AQI from the max of each particle's AQI\n",
    "    site = pd.merge(ozone_data, pm_data, how = 'outer', on = ['Date', 'Site_ID'])\n",
    "    site = site.fillna(0)\n",
    "    site = site[~((site[\"ozone_AQI\"] == 0) & (site[\"pm2.5_AQI\"] == 0))]\n",
    "    site['AQI'] = np.array([np.max(aqi_types) for aqi_types in site.loc[:, ['ozone_AQI', 'pm2.5_AQI']].to_numpy()])\n",
    "\n",
    "    # Cast the AQI into classes\n",
    "    site['class'] = pd.cut(site['AQI'], bins = aqi_cutoffs, labels = np.arange(6))\n",
    "    site['pm2.5_class'] = pd.cut(site['pm2.5_AQI'], bins = aqi_cutoffs, labels = np.arange(6))\n",
    "    site['AQI_class'] = pd.cut(site['AQI'], bins = aqi_cutoffs, labels = aqi_classes)\n",
    "    site['pm2.5_AQI_class'] = pd.cut(site['pm2.5_AQI'], bins = aqi_cutoffs, labels = aqi_classes)\n",
    "    \n",
    "    # Date cutoff to replicate our analysis\n",
    "    site = site.sort_values(['Date', 'Site_ID'])\n",
    "    site = site[(site[('Date')] <= '2020-10-11')]\n",
    "    site = site[~site[[\"Date\", \"Site_ID\"]].duplicated()]\n",
    "    \n",
    "    # Save final output file\n",
    "    site.to_csv(os.path.join(project_root_dir, 'final_data', 'all_sites_data.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48tLHh35xygg"
   },
   "outputs": [],
   "source": [
    "# Run the cleaning function\n",
    "clean_AQI_csv(aqi_classes, aqi_cutoffs, year_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3nJSkg+sSn1c79j/cpCHm",
   "collapsed_sections": [],
   "name": "epa_data_manipulation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
